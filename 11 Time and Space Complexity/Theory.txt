ğŸ“Œ Time Complexity & Space Complexity

1. Time Complexity: Time complexity measures how the running time of an algorithm grows with 
the size of the input n.

ğŸ”¹ Notations
Big-O (O): Worst case (upper bound).
Omega (Î©): Best case (lower bound).
Theta (Î˜): Average/exact bound.

ğŸ”¹ Common Time Complexities
O(1) â€“ Constant Time
Running time does not depend on input size.
Example: Accessing an element in an array.

O(log n) â€“ Logarithmic Time
Input size reduces by half each step.
Example: Binary Search.

O(n) â€“ Linear Time
Running time grows directly with input size.
Example: Traversing an array.

O(n log n) â€“ Linearithmic Time
Common in efficient divide-and-conquer algorithms.
Example: Merge Sort, Quick Sort (average case).

O(nÂ²) â€“ Quadratic Time
Usually from two nested loops.
Example: Bubble Sort, Insertion Sort.

O(2â¿) â€“ Exponential Time
Time doubles with each extra input element.
Example: Recursive Fibonacci (naive).

O(n!) â€“ Factorial Time
Extremely slow; tries all permutations.
Example: Traveling Salesman (brute force).

2. Space Complexity: Space complexity measures the amount of memory an algorithm needs as a 
function of input size n.

2. Trade-off Between Time & Space
Sometimes faster execution requires more memory, or less memory leads to slower execution.

Example:
Merge Sort â†’ O(n log n) time, O(n) space.
Quick Sort â†’ O(n log n) time, O(log n) space (in-place).

ğŸ“– Key Points for Revision
âœ… Always aim for the lowest time complexity possible.
âœ… Optimize space when working with limited memory.
âœ… Remember: O(1) < O(log n) < O(n) < O(n log n) < O(nÂ²) < O(2â¿) < O(n!)